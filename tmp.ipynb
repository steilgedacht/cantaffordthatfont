{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9ee68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from utils import GoogleFontsClassifier\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataloader import Datagenerator\n",
    "from train import Config\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20d14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/home/benjaminb/Dokumente/tmp/18/a.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0078d72",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "not supported for mode <built-in method mode of Tensor object at 0x7f2568611b50>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cropped_img \u001b[38;5;241m=\u001b[39m \u001b[43mImageOps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Preprocess for model (example: resize, normalize, to tensor)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_img \u001b[38;5;241m=\u001b[39m cropped_img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;28mint\u001b[39m( cropped_img\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m150\u001b[39m \u001b[38;5;241m/\u001b[39m cropped_img\u001b[38;5;241m.\u001b[39mheight), \u001b[38;5;241m150\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/PIL/ImageOps.py:633\u001b[0m, in \u001b[0;36minvert\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03mInvert (negate) the image.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m:param image: The image to invert.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m:return: An image.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    632\u001b[0m lut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mpoint(lut) \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_lut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlut\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/PIL/ImageOps.py:63\u001b[0m, in \u001b[0;36m_lut\u001b[0;34m(image, lut)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported for mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n",
      "\u001b[0;31mOSError\u001b[0m: not supported for mode <built-in method mode of Tensor object at 0x7f2568611b50>"
     ]
    }
   ],
   "source": [
    "\n",
    "# scale the image to a the height of 150px while keeping the aspect ratio\n",
    "image = image.resize((int( image.width * 150 / image.height), 150))\n",
    "\n",
    "image_np = np.array(image)\n",
    "\n",
    "# we check if the background is white or black and invert the image if it is white\n",
    "hist, bin_edges = np.histogram(image_np, bins=10)\n",
    "most_frequent_bins = np.argsort(hist)[::-1]\n",
    "if most_frequent_bins[0] < most_frequent_bins[1]:\n",
    "    image_np = 255 - image_np\n",
    "\n",
    "# normalize the image between 0 and 255\n",
    "image_np_norm = (image_np-np.min(image_np))/(np.max(image_np)-np.min(image_np)) * 255\n",
    "\n",
    "# fit the image to a 150x700px\n",
    "padded_tensor = np.ones((150, 700))\n",
    "h = min(image_np_norm.shape[1], 700)\n",
    "padded_tensor[:, :h] = image_np_norm[:, :h]\n",
    "image = padded_tensor\n",
    "image = torch.from_numpy(image).unsqueeze(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
